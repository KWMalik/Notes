The Exploit Arms Race - The Progressino of Attack and Defense in Software
Christien Rioux, Veracode

...

Things have gotten harder to write.
People are doing more due diligence.
It's not just manual consulting, there are product-sales, etc.

How is it that we were able to eliminate some of the low-hanging fruit out there?

Some of the exploits I thought were single-shot back in the day; have given way to much more complicated, multi-stage exploits. The initial exploit is small; but the payload is huge. Complicated trojans, remote bots, etc.

So in terms of the level of difficulting required in writing exploits going up, there's sort of a natural progression that has been going on. A 1-upmanship between attackers and defenders.

Attackers take a long time to select a target; they usually select low-hanging fruit. Defenders, have to respond as soon as possible.

Attackers only has to breach one point of interest; Defenders have to look at all points in their systems.

If attackers automate to find that one point interest, defenders need to do the same to scan for them.

CAPTION: the exploits that are valuable are the reliable; productized; ready to hack kind. Attackers only get one shot... if their exploit doesn't work, they will likely get caught.

The mechanisms now exist to defend against these and improve diligence. SDLCs, code review, etc.

There is sort of a balance- do you write them for the point of demonstrating proof-of-concept or a single instance? or do we spend time writing a valuable one.

It isn't always easy to test an exploit, you also have to test out where they will land. Often, people that report that an exploit works for them; have a system that is nearly the same as yours. In a sense you have to harden an exploit to handle more available scenarios. Image-loaders, trampolines, etc. to repurpose more points of entry for yourself.

For a reference, the black-hat underground market for vulnerability sales, a good remote-root can net an attacker $70,000 if it is reliable and zero-day valuable. That's what has been achieved as a security industry; making them more difficult to write; increasing the cost. But at the same time, the value of the spoils may also increase because of this.

** Who should be buying these?

** Who buys them:
  - consultancies
  - tools vendors
  - countries, intelligence agencies

the exploits are reliable because the attackers are winning the arms race against the defenders.
In some sense, some people are also looking to be the hero- they wish to prove that it was worth / justified to make the purchases and spend resources against a given scenario.

** What happens when there is no more 'low hanging fruit' ?

** Is this situation possible? is it likely?

There are new code technologies being released all the time. Also, there are weak users.

Buffer Overflows
Cross-Site Request Forging

Network Techniques such as:
Connection Hijacking
... (?)

Case Study: Internet Morris Worm, 1988
Affected 1/10th of the internet. VMS, Solaris 3. Targeted fingerd and attacked via buffer overflow.

Attack  2005 Brute Forcing
Defense 2006 DEP / Data Execution Prevention
  - works on x64 reliably, not as much on x86
Attack  2006 Return Orientation / Program-Heap spraying
  - Fundamental change in how programs are compiled
  - Giant callframe, return to next statement (REF: libc?)
Defense 2007 Automated Static Dataflow Analysis
  - Time consuming, would be unable to verify all programs in existence
Attack  2008 Manual and Automated Static Data Flow Analysis
Defense .... 100% Coverage Scanning, False Positive and False Negative Reduction Efforts
Attack  .... 3rd Party Channel Attacks, the Vulnerability Supply Chain
Defense .... Third Party Verification, Certificable Builds / Signing, Increased use of verifiable languages
Attack  .... Attackers move on? Are we done here?

Defenders need to fix as many vulnerabilities as possible.
To clean up the low-hanging-fruit, one needs to automate, since there's so much of it.

Can binary/source analysis scale? This remains to be seen.

How can one defend against an integer flow? You can't. Just fix the bug.

Binary Attacks
JAR Files
Zip/Compressed files
  - 42.zip - 42kb file that expands to 42pb
  - We don't have a good solution for integer overflows.
  - By structure, it can't be found. Typically found by CRCs.
Graphics/Movies

There is an entrenched mass of old libraries out there, that you can't even hope to examine or review.

JSP is a good bet.
ASP is good too.
Automation or a completely different model is required to ensure that bad code can't create design-level flaws like XSS.

Some languages are also problematic.

C - Buffer/Integer overflows
PHP - Cross-site scripting

Think of all the IIS 5.0 vulnerabilities.
  - Unicode problems
  - Path Manipulations
  - tracking and exploiting URL-encoding layers, forwards/backwards

Unix Issues
  - Inheirited handles
  - chmod root
  - pipes

There are also political issues- who owns the rights and responsibilities for files that are weak; shipped in package with other products.

** Considerations

do you really need that old code?
what is the cost of maintenance with security risks factored in?

a long time ago, @stake taught people on 'Return on Security Investment'

People should have listened.

Example: Rewrite compilation of C/C++ with two separate stacks. One stack for program flow, One stack for data flow (alloc, stack variables)

Example: Add another stack/heap for function pointers- sorted or organized storage, preventing programs from clobbering points of execution.

Q: You've spoken a lot about desktop exploits. What do you think of mobile exploits?
A: With the proliferation of app stores, the different models by which you can pickup software from outside sources. What people mostly want from mobile phones, is your data. Taking over a mobile phone isn't as useful. But- one attack that would be deadly if it succeeded, is something where you could take over a host by a poisoned phone; on recharge or sync. Also deadly would be a coordinated pair of poisoned programs. Such a pair could constantly await a hole for you to expose valuable information (bank info, cc).
A: The blackberry platform did get popped, despite its good security. One place they got popped was in the PDF handler on the enterprise server.

Q: You talked about automating the discovery of vulnerabilities. Can you give us more details?
A: I know of people who have written dirty, quick, yet cheap fuzzers to go over a program; to feed inputs and observe whether bad things happen. Could also setup fuzzing observers... run a bunch of programs on scripts, setup the fuzzers, and wait for a crash. On the static side; things are quite more advanced, eg. There are decompilers. Can handle mixing of multiple types of binaries and libraries. Mixed apps like Java + C + ???. Automating the process of finding vulnerabilities is an important activity for both attackers and defenders.

Q: Have you ever seen a buffer overflow for Java lately? And one that was able to take over a box?
A: Lately the JVMs have been good. One such instance was Google writing their own JVM (delvik). For legal reasons, they had to minimally implement from scratch; something to run Java. They butted heads with Sun; but this created an opportunity. When you rewrite something from scratch; you reset the bug count to zero. On the other hand, pure Java programs are pretty good. This is in comparison to desktop java apps, where you know there will be a shim over the OS. You know that there could be a chance to hook an exploit that leaks down to that level. I'm not convinced we're out of the woods in terms of VMs, Hypervisors, etc. Every time someone writes a driver to try and do something custom; reaching into the guest space; someone else manages to find a hole. I'm waiting for the first time where a Hypervisor pages a VM to disk- and a guest has a non-conformant NTFS partition or VM-Physical memory mapping; and this causes an exploit.

